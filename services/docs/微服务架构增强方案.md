# 实施计划：微服务架构增强方案

**规划周期**: 1+2+4 = 7周 (分三个阶段)  
**目标评分提升**: 70 → 90分 (提升20分)  
**优先级**: 快速可观测性 → 事件驱动 → 企业功能

---

## Phase 1: 快速提升（第1-2周）| 目标 +20分

**TL;DR**: 补齐可观测性三角形 (Logs + Traces + Metrics)，使系统从"黑盒"变"透明"

### 阶段目标
- ✅ 添加链路追踪（Skywalking）- 解决"请求到底去哪了"
- ✅ 添加指标收集（Prometheus）- 解决"系统状态怎样"
- ✅ 添加日志聚合（Logback + ELK）- 解决"哪里出错了"

### 阶段收益
```
评分提升:  70 → 82分 (+12分)
生产就绪度: 45% → 70%
故障定位时间: 从小时级 → 分钟级
```

---

## 任务1: 集成Skywalking链路追踪

**完成标准**: 
- [ ] 5个服务都有链路追踪
- [ ] 跨服务调用链路完整
- [ ] 响应时间分解可视
- [ ] 错误采样率100%

### Step 1.1: Skywalking服务器部署

**文件**: `services/docker-compose.yml`

**操作**:
```yaml
# 在 docker-compose.yml 中添加 Skywalking

skywalking-oap:
  image: apache/skywalking-oap-server:9.7.0
  container_name: skywalking-oap
  environment:
    SW_STORAGE: h2
    SW_H2_DRIVER: org.h2.Driver
  ports:
    - "11800:11800"  # gRPC 采集端口
    - "12800:12800"  # HTTP 查询端口
  networks:
    - infra-network

skywalking-ui:
  image: apache/skywalking-ui:9.7.0
  container_name: skywalking-ui
  ports:
    - "8899:8080"
  environment:
    SW_OAP_ADDRESS: skywalking-oap:12800
  depends_on:
    - skywalking-oap
  networks:
    - infra-network
```

**验证**: 
```bash
# 启动
cd services && docker-compose up skywalking-oap skywalking-ui

# 验证 - 浏览器打开
http://localhost:8899
```

### Step 1.2: 更新父POM - 添加Skywalking依赖

**文件**: `services/pom.xml`

**在 `<dependencyManagement>` 中添加**:
```xml
<!-- Skywalking Java Agent (在 BOM 管理版本) -->
<dependency>
    <groupId>org.apache.skywalking</groupId>
    <artifactId>apm-toolkit-trace</artifactId>
    <version>9.7.0</version>
</dependency>

<!-- Micrometer Tracing (Spring Boot 3.x 官方推荐) -->
<dependency>
    <groupId>io.micrometer</groupId>
    <artifactId>micrometer-tracing-bom</artifactId>
    <version>1.2.0</version>
    <type>pom</type>
    <scope>import</scope>
</dependency>
```

### Step 1.3: 更新每个服务 - 添加运行时Agent配置

**修改每个服务的启动脚本** (在 Makefile 或启动命令中)

**文件**: `services/Makefile`

**添加Agent参数** (Java 启动命令):
```bash
# 在 dev target 中修改 maven spring-boot:run 命令

java -javaagent:./skywalking-agent/skywalking-agent.jar \
     -Dskywalking.agent.service_name=${SERVICE_NAME} \
     -Dskywalking.collector.backend_service=localhost:11800 \
     -cp ...classpath...
```

**实际Makefile改法**:
```makefile
dev:
	@echo "Starting all microservices with Skywalking..."
	# 需要先下载 Skywalking agent
	wget -q https://archive.apache.org/dist/skywalking/9.7.0/apache-skywalking-java-agent-9.7.0.tar.gz || true
	tar -xzf apache-skywalking-java-agent-9.7.0.tar.gz || true
	
	# 启动各服务（示例 auth-service）
	cd auth-service && \
	mvn spring-boot:run \
	  -Dspring-boot.run.jvmArguments="\
	    -javaagent:../skywalking-agent/skywalking-agent.jar \
	    -Dskywalking.agent.service_name=auth-service \
	    -Dskywalking.collector.backend_service=localhost:11800" &
```

**简化方案**（推荐）: 使用Docker自动注入
```dockerfile
# 在每个 Dockerfile 中

FROM openjdk:17-slim
RUN wget -q https://archive.apache.org/dist/skywalking/9.7.0/apache-skywalking-java-agent-9.7.0.tar.gz && \
    tar -xzf apache-skywalking-java-agent-9.7.0.tar.gz && \
    rm apache-skywalking-java-agent-9.7.0.tar.gz

ENV JAVA_OPTS="-javaagent:/app/skywalking-agent/skywalking-agent.jar \
              -Dskywalking.collector.backend_service=skywalking-oap:11800 \
              -Dskywalking.agent.service_name=${SERVICE_NAME}"

COPY target/auth-service.jar app.jar
ENTRYPOINT exec java $JAVA_OPTS -jar app.jar
```

### Step 1.4: 验证链路追踪

**验证清单**:
```bash
# 1. 启动Skywalking
make up  # 启动基础设施

# 2. 启动所有服务
make dev-full

# 3. 发送测试请求
curl -X POST http://localhost:8080/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username":"admin","password":"Test@1234"}'

# 4. 打开Skywalking UI查看链路
# http://localhost:8899 → Services → auth-service → 拓扑图

# 5. 检查追踪
# 应该看到: API Gateway → Auth Service → User Service 完整链路
```

**预期效果**:
```
✓ Skywalking UI显示5个服务
✓ 拓扑图显示完整调用关系
✓ 单个请求链路时间分解清晰
✓ 错误请求红色标记
```

**工作量**: 2-3天

---

## 任务2: 集成Prometheus + Grafana指标收集

**完成标准**:
- [ ] Prometheus采集5个服务的指标
- [ ] Grafana仪表盘展示关键指标（CPU/内存/请求数/响应时间）
- [ ] 告警规则配置
- [ ] 历史数据保留30天

### Step 2.1: 更新服务POM - 添加Micrometer Prometheus

**文件**: `services/pom.xml` - 在 `<dependencyManagement>` 中

```xml
<!-- Prometheus 指标导出 -->
<dependency>
    <groupId>io.micrometer</groupId>
    <artifactId>micrometer-registry-prometheus</artifactId>
    <version>1.12.0</version>
</dependency>

<!-- Spring Boot Actuator（已有，确保包含） -->
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
```

### Step 2.2: 更新application.yml - 开启Prometheus端点

**文件**: `services/auth-service/src/main/resources/application.yml` (以及其他服务)

```yaml
# 在每个服务的 application.yml 中添加

management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus  # 添加 prometheus
  metrics:
    export:
      prometheus:
        enabled: true
    tags:
      application: ${spring.application.name}  # 添加应用名标签
```

**对其他服务重复添加**: user-service, order-service, chat-service, api-gateway

### Step 2.3: Docker Compose - 添加Prometheus & Grafana

**文件**: `docker-compose.yml`

```yaml
prometheus:
  image: prom/prometheus:latest
  container_name: prometheus
  ports:
    - "9090:9090"
  volumes:
    - ./docker/prometheus.yml:/etc/prometheus/prometheus.yml
    - prometheus_data:/prometheus
  command:
    - '--config.file=/etc/prometheus/prometheus.yml'
    - '--storage.tsdb.path=/prometheus'
    - '--storage.tsdb.retention.time=30d'  # 保留30天数据
  networks:
    - infra-network
  depends_on:
    - api-gateway  # 等待服务启动

grafana:
  image: grafana/grafana:latest
  container_name: grafana
  ports:
    - "3000:3000"
  environment:
    GF_SECURITY_ADMIN_PASSWORD: admin  # 改为强密码
    GF_INSTALL_PLUGINS: grafana-piechart-panel
  volumes:
    - grafana_data:/var/lib/grafana
    - ./docker/grafana/provisioning:/etc/grafana/provisioning
  networks:
    - infra-network
  depends_on:
    - prometheus

volumes:
  prometheus_data:
  grafana_data:
```

### Step 2.4: 创建Prometheus配置

**文件**: `docker/prometheus.yml` (新建)

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    monitor: 'microservices'

scrape_configs:
  # API Gateway
  - job_name: 'api-gateway'
    static_configs:
      - targets: ['localhost:8080']
    metrics_path: '/actuator/prometheus'

  # Auth Service
  - job_name: 'auth-service'
    static_configs:
      - targets: ['localhost:8002']
    metrics_path: '/actuator/prometheus'

  # User Service
  - job_name: 'user-service'
    static_configs:
      - targets: ['localhost:8001']
    metrics_path: '/actuator/prometheus'

  # Order Service
  - job_name: 'order-service'
    static_configs:
      - targets: ['localhost:8003']
    metrics_path: '/actuator/prometheus'

  # Chat Service
  - job_name: 'chat-service'
    static_configs:
      - targets: ['localhost:8004']
    metrics_path: '/actuator/prometheus'

# 告警规则
rule_files:
  - '/etc/prometheus/alert-rules.yml'

alerting:
  alertmanagers:
    - static_configs:
        - targets: []  # 后续可配置告警管理器
```

### Step 2.5: 创建Grafana仪表盘

**文件**: `docker/grafana/provisioning/dashboards/microservices.json` (新建)

```json
{
  "dashboard": {
    "title": "Microservices Overview",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [
          {
            "expr": "rate(http_requests_total[1m])"
          }
        ]
      },
      {
        "title": "Response Time (P95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, http_request_duration_seconds_bucket)"
          }
        ]
      },
      {
        "title": "JVM Memory Usage",
        "targets": [
          {
            "expr": "jvm_memory_used_bytes / jvm_memory_max_bytes"
          }
        ]
      },
      {
        "title": "Error Rate",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[1m])"
          }
        ]
      }
    ]
  }
}
```

### Step 2.6: 验证Prometheus & Grafana

```bash
# 1. 启动docker服务
docker-compose up prometheus grafana -d

# 2. 验证Prometheus
# http://localhost:9090/targets → 所有5个服务应显示"Up"

# 3. 验证Grafana
# http://localhost:3000 
# 用户名: admin, 密码: admin

# 4. 导入仪表盘
# Settings → Data Sources → Add Prometheus (http://prometheus:9090)
# Dashboards → Import → 上传 microservices.json

# 5. 测试流量
curl -X POST http://localhost:8080/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username":"admin","password":"Test@1234"}' -v

# 应该在Grafana看到指标变化
```

**工作量**: 2-3天

---

## 任务3: 集成ELK日志聚合 (Elasticsearch + Logstash + Kibana)

**完成标准**:
- [ ] 所有服务日志统一收集
- [ ] 日志可按服务/级别/关键字搜索
- [ ] 错误堆栈跟踪可视化
- [ ] 日志保留30天

### Step 3.1: 更新每个服务POM - 添加Logback JSON编码器

**文件**: `services/pom.xml` - 在 `<dependencyManagement>` 中

```xml
<!-- Logback JSON 编码器 - 便于日志聚合系统解析 -->
<dependency>
    <groupId>net.logstash.logback</groupId>
    <artifactId>logstash-logback-encoder</artifactId>
    <version>7.4</version>
</dependency>
```

### Step 3.2: 为每个服务创建logback-spring.xml

**文件**: `services/auth-service/src/main/resources/logback-spring.xml` (新建，其他服务重复)

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <springProperty name="APP_NAME" source="spring.application.name" />
    <springProperty name="LOG_FILE" source="logging.file.name" defaultValue="logs/${APP_NAME}.log" />
    
    <!-- 控制台输出 (本地开发) -->
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>
    
    <!-- JSON文件输出 (用于ELK采集) -->
    <appender name="FILE_JSON" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${LOG_FILE}</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
            <fileNamePattern>logs/${APP_NAME}-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
            <maxFileSize>100MB</maxFileSize>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <customFields>{"application":"${APP_NAME}","environment":"production"}</customFields>
        </encoder>
    </appender>
    
    <!-- TCP输出到Logstash (生产环境) -->
    <appender name="LOGSTASH" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>localhost:5000</destination>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <customFields>{"application":"${APP_NAME}"}</customFields>
        </encoder>
    </appender>
    
    <!-- 按环境配置 -->
    <springProfile name="local">
        <root level="DEBUG">
            <appender-ref ref="CONSOLE" />
            <appender-ref ref="FILE_JSON" />
        </root>
    </springProfile>
    
    <springProfile name="prod">
        <root level="INFO">
            <appender-ref ref="FILE_JSON" />
            <appender-ref ref="LOGSTASH" />
        </root>
    </springProfile>
    
    <logger name="org.springframework" level="INFO" />
    <logger name="org.apache.dubbo" level="INFO" />
</configuration>
```

**对其他服务重复**: user-service, order-service, chat-service, api-gateway

### Step 3.3: Docker Compose - 添加ELK Stack

**文件**: `docker-compose.yml`

```yaml
# Elasticsearch
elasticsearch:
  image: docker.elastic.co/elasticsearch/elasticsearch:8.10.0
  container_name: elasticsearch
  environment:
    - discovery.type=single-node
    - xpack.security.enabled=false
    - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
  ports:
    - "9200:9200"
  volumes:
    - elasticsearch_data:/usr/share/elasticsearch/data
  networks:
    - infra-network

# Logstash
logstash:
  image: docker.elastic.co/logstash/logstash:8.10.0
  container_name: logstash
  volumes:
    - ./docker/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
  ports:
    - "5000:5000/tcp"  # TCP输入 (应用日志)
    - "9600:9600"      # Monitoring API
  environment:
    - "LS_JAVA_OPTS=-Xmx256m -Xms256m"
  depends_on:
    - elasticsearch
  networks:
    - infra-network

# Kibana
kibana:
  image: docker.elastic.co/kibana/kibana:8.10.0
  container_name: kibana
  environment:
    - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
  ports:
    - "5601:5601"
  depends_on:
    - elasticsearch
  networks:
    - infra-network

volumes:
  elasticsearch_data:
```

### Step 3.4: 创建Logstash配置

**文件**: `docker/logstash.conf` (新建)

```conf
input {
  tcp {
    port => 5000
    codec => json
  }
}

filter {
  # 解析应用名
  mutate {
    add_field => { "[@metadata][index_name]" => "logs-%{+YYYY.MM.dd}" }
  }
  
  # 错误日志标记
  if [level] == "ERROR" or [level] == "WARN" {
    mutate {
      add_tag => [ "alert" ]
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index_name]}"
  }
  
  # 控制台输出（调试用）
  stdout {
    codec => rubydebug
  }
}
```

### Step 3.5: 验证ELK日志聚合

```bash
# 1. 启动ELK
docker-compose up elasticsearch logstash kibana -d

# 2. 等待Elasticsearch启动（约30秒）
sleep 30

# 3. 验证Elasticsearch
curl http://localhost:9200/_cat/health

# 4. 验证Kibana
# http://localhost:5601

# 5. 启动应用，生成日志
make dev

# 6. 发送测试请求生成日志
for i in {1..10}; do
  curl -X POST http://localhost:8080/api/auth/login \
    -H "Content-Type: application/json" \
    -d '{"username":"admin","password":"Test@1234"}' 2>/dev/null
done

# 7. 在Kibana查看日志
# 浏览器打开 http://localhost:5601
# 点击 "Discover" → 应该看到incoming logs

# 8. 创建索引模式
# Stack Management → Index Patterns → Create index pattern
# 输入: logs-*
# Time field: @timestamp
```

**预期效果**:
```
✓ Kibana显示最新日志
✓ 日志包含应用名、级别、消息、堆栈跟踪
✓ 可按服务/关键字搜索
✓ 错误高亮显示
```

**工作量**: 2天

---

## Phase 1 检查清单

```
快速提升阶段完成度检查 (Week 1-2)

任务1: Skywalking链路追踪
  □ Skywalking服务器部署 (docker-compose.yml)
  □ 5个服务POM更新
  □ 服务启动Command更新
  □ 链路验证 (http://localhost:8899)
  □ 完成度: ___%

任务2: Prometheus + Grafana指标
  □ Micrometer依赖添加
  □ actuator/prometheus端点开启
  □ Prometheus服务部署 (docker-compose.yml)
  □ Grafana数据源配置
  □ 仪表盘创建和验证
  □ 完成度: ___%

任务3: ELK日志聚合
  □ Logback JSON编码器依赖
  □ logback-spring.xml配置 (5个服务)
  □ ELK Stack部署
  □ Logstash配置
  □ Kibana索引模式创建
  □ 完成度: ___%

验收标准:
  ✓ 3个可观测性支柱完整
  ✓ 所有5个服务都可监控
  ✓ 问题定位时间 < 5分钟
  ✓ 评分提升 70 → 82分
```

---

## Phase 2: 事件驱动架构（第3-4周）| 目标 +15分

**TL;DR**: 添加Kafka事件总线，支持异步处理和最终一致性

### 阶段目标
- ✅ Kafka Broker部署
- ✅ User创建事件发布-订阅
- ✅ Order异步处理
- ✅ Chat会话事件持久化

### 阶段收益
```
评分提升:  82 → 92分 (+10分)
架构灵活性: 从同步 → 异步+同步混合
可扩展性: 新服务无需修改现有服务
```

### Step 2.1: 父POM - 添加Kafka依赖

**文件**: `services/pom.xml`

```xml
<!-- Kafka -->
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
    <version>3.2.0</version>
</dependency>

<!-- Avro/Protocol Buffers for 消息schema (可选) -->
<dependency>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro</artifactId>
    <version>1.11.3</version>
</dependency>
```

### Step 2.2: User Service - 发布"UserCreated"事件

**新建文件**: `services/user-service/src/main/java/com/example/user/event/UserCreatedEvent.java`

```java
package com.example.user.event;

import lombok.AllArgsConstructor;
import lombok.Data;
import java.time.LocalDateTime;

@Data
@AllArgsConstructor
public class UserCreatedEvent {
    private Long userId;
    private String username;
    private String email;
    private LocalDateTime createdAt;
    private String source; // "registration" / "admin-import" / "saas-onboarding"
}
```

**新建文件**: `services/user-service/src/main/java/com/example/user/event/UserEventPublisher.java`

```java
package com.example.user.event;

import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Component;
import lombok.RequiredArgsConstructor;

@Component
@RequiredArgsConstructor
public class UserEventPublisher {
    private final KafkaTemplate<String, UserCreatedEvent> kafkaTemplate;
    private static final String TOPIC = "user-created-events";

    public void publishUserCreated(UserCreatedEvent event) {
        kafkaTemplate.send(TOPIC, String.valueOf(event.getUserId()), event);
    }
}
```

**修改**: `services/user-service/src/main/java/com/example/user/service/UserService.java`

```java
// 在 registerUser() 方法中添加事件发布

@Transactional
public User registerUser(RegisterRequest request) {
    // 1. 创建用户
    User user = new User(request.getUsername(), request.getEmail());
    userRepository.save(user);
    
    // 2. 发布事件（异步处理订阅）
    userEventPublisher.publishUserCreated(
        new UserCreatedEvent(
            user.getId(),
            user.getUsername(),
            user.getEmail(),
            LocalDateTime.now(),
            "registration"
        )
    );
    
    return user;
}
```

### Step 2.3: Auth Service/Order Service - 订阅事件

**新建文件**: `services/auth-service/src/main/java/com/example/auth/event/UserEventListener.java`

```java
package com.example.auth.event;

import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Component;
import com.example.user.event.UserCreatedEvent;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;

@Component
@Slf4j
@RequiredArgsConstructor
public class UserEventListener {
    
    /**
     * 监听用户创建事件 - 异步初始化认证相关资源
     */
    @KafkaListener(topics = "user-created-events", groupId = "auth-service")
    public void handleUserCreated(UserCreatedEvent event) {
        log.info("Auth Service收到用户创建事件: {}", event.getUserId());
        
        // 初始化认证相关配置（如MFA、登录历史）
        initializeAuthConfig(event.getUserId());
        
        // 发送欢迎邮件
        sendWelcomeEmail(event.getEmail());
    }
    
    private void initializeAuthConfig(Long userId) {
        // 创建初始认证配置
    }
    
    private void sendWelcomeEmail(String email) {
        // 异步发送欢迎邮件
    }
}
```

**同样在 Order Service 添加**:
```java
@KafkaListener(topics = "user-created-events", groupId = "order-service")
public void handleUserCreated(UserCreatedEvent event) {
    // 初始化用户订单相关配置
    initializeOrderAccount(event.getUserId());
}
```

### Step 2.4: application.yml 配置Kafka

**文件**: `services/user-service/src/main/resources/application.yml`

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      acks: all  # 确保消息被所有副本接收
      retries: 3
    consumer:
      bootstrap-servers: localhost:9092
      group-id: ${spring.application.name}-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        spring.json.type.mapping: "userCreatedEvent:com.example.user.event.UserCreatedEvent"
```

### Step 2.5: Docker Compose - 添加Kafka

**文件**: `docker-compose.yml`

```yaml
# Kafka + Zookeeper
zookeeper:
  image: confluentinc/cp-zookeeper:7.5.0
  container_name: zookeeper
  environment:
    ZOOKEEPER_CLIENT_PORT: 2181
    ZOOKEEPER_SYNC_LIMIT: 2
    ZOOKEEPER_INIT_LIMIT: 5
  ports:
    - "2181:2181"
  networks:
    - infra-network

kafka:
  image: confluentinc/cp-kafka:7.5.0
  container_name: kafka
  ports:
    - "9092:9092"
  environment:
    KAFKA_BROKER_ID: 1
    KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://kafka:9092
    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
    KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
  depends_on:
    - zookeeper
  networks:
    - infra-network

# Kafka UI (可视化管理工具)
kafka-ui:
  image: provectuslabs/kafka-ui:latest
  container_name: kafka-ui
  ports:
    - "8085:8080"
  environment:
    KAFKA_CLUSTERS_0_NAME: local
    KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
    KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
  depends_on:
    - kafka
  networks:
    - infra-network
```

### Step 2.6: 验证事件驱动

```bash
# 1. 启动Kafka
docker-compose up zookeeper kafka kafka-ui -d
sleep 30

# 2. 创建主题（可选，auto-create已启用）
kafka-topics --create --topic user-created-events --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1

# 3. 启动所有服务
make dev

# 4. 发送用户创建请求
curl -X POST http://localhost:8080/api/users/register \
  -H "Content-Type: application/json" \
  -d '{
    "username": "testuser",
    "email": "test@example.com",
    "password": "Test@1234"
  }'

# 5. 在Kafka UI查看消息
# http://localhost:8085
# Topics → user-created-events → Messages

# 6. 验证订阅者收到事件
# 查看 auth-service 和 order-service 日志
docker logs auth-service | grep "收到用户创建事件"
docker logs order-service | grep "收到用户创建事件"
```

**工作量**: 1-2周

---

## Phase 3: 企业功能增强（第5-8周）| 目标 +15分

### 概览
包含以下几个模块，详见详细规划文档：

1. **分布式事务（Seata）** - 解决跨服务事务一致性
2. **工作流引擎（Activiti）** - 支持流程自定义
3. **多租户隔离** - 企业级数据隔离
4. **Kubernetes部署** - 云原生编排

### Phase 3.1: 分布式事务（Seata）

**目标**: 解决Order创建时User余额扣除的分布式事务问题

**核心步骤**:
- Seata Server部署
- TC/TM/RM配置
- 事务注解标记
- 补偿流程设计

### Phase 3.2: 工作流引擎（Activiti）

**目标**: 订单审批/权限申请等工作流

**核心步骤**:
- Activiti集成
- BPMN流程定义
- 流程实例管理
- 任务分派与追踪

### Phase 3.3: 多租户隔离

**目标**: 支持SaaS模式的数据隔离

**核心步骤**:
- 租户上下文传递
- 数据路由与隔离
- 资源配额管理
- 租户自助管理

### Phase 3.4: Kubernetes部署

**目标**: 云原生部署与自动扩展

**核心步骤**:
- Dockerfile优化
- Service/Ingress配置
- 资源请求与限制
- 自动水平扩展

---

## 完整时间表

```
Week 1-2  | Phase 1: 快速可观测性
├─ W1-D1-3: Skywalking  ■■■
├─ W1-D4-7: Prometheus  ■■■  
└─ W2:     ELK Kibana   ■■■
   评分: 70 → 82

Week 3-4  | Phase 2: 事件驱动
├─ W3:     Kafka部署    ■■
├─ W3-4:   事件设计     ■■■
└─ W4:     端到端测试   ■
   评分: 82 → 92

Week 5-8  | Phase 3: 企业功能
├─ W5-6:   Seata分布式事务
├─ W6-7:   工作流引擎
├─ W7:     多租户隔离
└─ W8:     Kubernetes
   评分: 92 → 100+
```

---

## 成功标志

### Phase 1完成标志
- [ ] 5个应用都显示在Skywalking服务列表
- [ ] Grafana仪表盘显示实时指标
- [ ] Kibana可搜索所有日志
- [ ] 故障定位时间 < 5分钟
- **评分**: 70 → 82分

### Phase 2完成标志
- [ ] Kafka topics已创建
- [ ] 新用户创建自动触发事件
- [ ] 订阅者成功接收事件
- [ ] 事件处理错误自动记录
- **评分**: 82 → 92分

### Phase 3完成标志
- [ ] Seata配置分布式事务
- [ ] 跨服务事务自动回滚
- [ ] Activiti工作流可配置
- [ ] 支持多租户隔离
- [ ] K8s部署成功
- **评分**: 92 → 100+分

---

## 资源清单

### 需要新建文件
```
services/
├── docker/
│   ├── prometheus.yml               [新建]
│   ├── logstash.conf                [新建]
│   ├── alert-rules.yml              [新建]
│   └── grafana/
│       ├── provisioning/
│       │   ├── datasources/
│       │   │   └── prometheus.yml   [新建]
│       │   └── dashboards/
│       │       └── microservices.json [新建]
│       
├── auth-service/src/main/resources/
│   └── logback-spring.xml           [新建]
│
├── user-service/src/main/
│   ├── resources/logback-spring.xml [新建]
│   └── java/com/example/user/event/ [新建文件夹]
│       ├── UserCreatedEvent.java
│       ├── UserEventPublisher.java
│       └── UserEventListener.java
│
├── order-service/src/main/
│   ├── resources/logback-spring.xml [新建]
│   └── java/com/example/order/event/
│       └── UserEventListener.java
│
├── chat-service/src/main/resources/
│   └── logback-spring.xml           [新建]
│
├── api-gateway/src/main/resources/
│   └── logback-spring.xml           [新建]
│
└── docs/
    ├── IMPLEMENTATION_PLAN.md       [本文件]
    ├── ENHANCEMENT_ROADMAP.md       [待创建]
    └── KAFKA_BEST_PRACTICES.md      [待创建]
```

### 需要修改的文件
```
services/
├── pom.xml                          [修改] 添加依赖
├── docker-compose.yml               [修改] 添加ELK/Prometheus/Kafka/Grafana
├── Makefile                         [修改] Agent启动参数
│
├── auth-service/
│   ├── pom.xml                      [修改] JSON encoder
│   └── src/main/resources/application.yml [修改] Kafka/Logback
│
├── user-service/
│   ├── pom.xml                      [修改]
│   └── src/main/resources/application.yml [修改]
│
├── order-service/
│   ├── pom.xml                      [修改]
│   └── src/main/resources/application.yml [修改]
│
├── chat-service/
│   ├── pom.xml                      [修改]
│   └── src/main/resources/application.yml [修改]
│
└── api-gateway/
    ├── pom.xml                      [修改]
    └── src/main/resources/application.yml [修改]
```

---

## 注意事项

### 版本兼容性
```
✓ Java 17 + Spring Boot 3.2.0 + Spring Cloud 2023.0.0
✓ Skywalking 9.7.0 + micrometer-tracing 1.2.0
✓ Prometheus 2.47+ + Grafana 10+
✓ ELK 8.10+
✓ Kafka 7.5.0
→ 推荐在隔离环境测试版本升级
```

### 性能考虑
- Skywalking采样率：100%（开发） → 10-20%（生产）
- Prometheus数据保留：30天（推荐）
- Elasticsearch分片：3分片×1副本 → 生产环境3×3
- Kafka分区：3个（开发） → 更多（生产按吞吐调整）

### 安全加固
```
必做事项:
□ Elasticsearch启用认证 (xpack.security.enabled=true)
□ Kafka启用SASL认证
□ Prometheus隐藏在网关后（不公网暴露）
□ Kibana/Grafana改强密码
□ 采样数据脱敏（PII处理）
```

### 故障排查
```
常见问题:

Q: Skywalking读不到链路
A: 检查 -javaagent 参数，确保 skywalking-agent.jar 路径正确

Q: Prometheus采集为 DOWN
A: 检查防火墙，确保 /actuator/prometheus 可访问

Q: Kibana没有日志
A: 检查 logstash.conf tcp port 是否正确，logback配置是否生效

Q: Kafka消费者收不到消息
A: 检查 groupId 是否重复，bootstrap-servers 是否正确
```

---

## 成功指标 (KPI)

```
Week 2 底线:
  ✓ 评分 70 → 82 (+12分)
  ✓ 5个服务链路完整追踪
  ✓ 系统监控盲点消除
  
Week 4 目标:
  ✓ 评分 82 → 92 (+10分)
  ✓ 事件驱动架构就绪
  ✓ 异步处理支持
  
Week 8 目标:
  ✓ 评分 92 → 100+ 
  ✓ 企业级功能完整
  ✓ 可直接用于中型企业
```

---

**计划审核**: 完成度 __/100  
**批准人**: ________________  
**批准日期**: ________________  
**预计完成**: 2026-04-02 (8周)
